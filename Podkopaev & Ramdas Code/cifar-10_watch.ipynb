{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including torch, torchvision, and other required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CIFAR-10 and CIFAR-10-Corrupted Datasets\n",
    "Load the CIFAR-10 dataset using torchvision.datasets and CIFAR-10-Corrupted dataset from the appropriate source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Download CIFAR-10-C dataset\n",
    "# url = \"https://zenodo.org/records/2535967/files/CIFAR-10-C.tar?download=1\"\n",
    "# filename = \"./data/CIFAR-10-C.tar\"\n",
    "# urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "# # Extract the dataset\n",
    "# with tarfile.open(filename, 'r') as tar_ref:\n",
    "#     tar_ref.extractall(\"./data\")\n",
    "# os.remove(filename)\n",
    "\n",
    "# Define transformations for the training and test sets\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# Load CIFAR-10-Corrupted dataset\n",
    "# corrupted_testset = torchvision.datasets.ImageFolder(root='./data/CIFAR-10-C', transform=transform)\n",
    "# corrupted_testloader = torch.utils.data.DataLoader(corrupted_testset, batch_size=100,\n",
    "#                                                    shuffle=False, num_workers=2)\n",
    "\n",
    "# Define classes\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pdb\n",
    "\n",
    "class NpyCIFAR10CDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset for CIFAR-10-C if data is stored in .npy files.\n",
    "    Expects shape (N, 32, 32, 3) for images and shape (N,) for labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, images_path, labels_path, severity, transform=None):\n",
    "        super().__init__()\n",
    "        self.images = np.load(images_path)[(severity - 1)*10000: severity*10000]   # shape: (N, 32, 32, 3)\n",
    "        self.labels = np.load(labels_path)[(severity - 1)*10000: severity*10000]  # shape: (N,)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]    # shape: (32, 32, 3)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert to float tensor\n",
    "        img_tensor = torch.from_numpy(img).float()  # shape: (32, 32, 3)\n",
    "\n",
    "        # Permute to (C, H, W) => (3, 32, 32)\n",
    "        img_tensor = img_tensor.permute(2, 0, 1)\n",
    "\n",
    "        # If there's a transform, apply it (e.g., normalization)\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img_tensor)\n",
    "\n",
    "        return img_tensor, label\n",
    "\n",
    "def get_cifar10c_loader(corruption_type='fog', severity=5, batch_size=64):\n",
    "    \"\"\"\n",
    "    Create a DataLoader for CIFAR-10-C (single corruption or combined),\n",
    "    assuming .npy files for images and labels.\n",
    "    \"\"\"\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std  = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    dataset = NpyCIFAR10CDataset(\n",
    "        images_path=f'/cis/home/xhan56/code/wtr/data/CIFAR-10-C/{corruption_type}.npy',\n",
    "        labels_path='/cis/home/xhan56/code/wtr/data/CIFAR-10-C/labels.npy',\n",
    "        severity=5,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the MLP Model\n",
    "Define a multi-layer perceptron (MLP) model using torch.nn.Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP Model\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple MLP for CIFAR-10 classification.\n",
    "    Input: (N, 3, 32, 32)\n",
    "    Output: (N, 10) for 10 classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=3*32*32, hidden_size=1024, num_classes=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (N, 3, 32, 32)\n",
    "        # Flatten to (N, 3*32*32 = 3072)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Training and Evaluation Functions\n",
    "Set up functions to train and evaluate the model, including loss calculation and optimization steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up Training and Evaluation Functions\n",
    "\n",
    "def train_one_epoch(model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(dim=1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, device, data_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(dim=1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model on CIFAR-10\n",
    "Train the MLP model on the CIFAR-10 dataset using the defined training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Train Loss: 1.6656, Train Acc: 40.89%\n",
      "   Clean Test Loss: 1.5365, Clean Acc: 45.08%\n",
      "   Corrupted Test Loss: 264.2964, Corrupted Acc: 16.59%\n",
      "Epoch [2/10] - Train Loss: 1.4688, Train Acc: 47.93%\n",
      "   Clean Test Loss: 1.4846, Clean Acc: 47.80%\n",
      "   Corrupted Test Loss: 598.6357, Corrupted Acc: 12.93%\n",
      "Epoch [3/10] - Train Loss: 1.3585, Train Acc: 52.07%\n",
      "   Clean Test Loss: 1.4057, Clean Acc: 50.62%\n",
      "   Corrupted Test Loss: 598.3467, Corrupted Acc: 15.35%\n",
      "Epoch [4/10] - Train Loss: 1.2777, Train Acc: 54.96%\n",
      "   Clean Test Loss: 1.4403, Clean Acc: 50.36%\n",
      "   Corrupted Test Loss: 259.4511, Corrupted Acc: 13.99%\n",
      "Epoch [5/10] - Train Loss: 1.2069, Train Acc: 57.25%\n",
      "   Clean Test Loss: 1.4333, Clean Acc: 50.96%\n",
      "   Corrupted Test Loss: 667.0127, Corrupted Acc: 11.74%\n",
      "Epoch [6/10] - Train Loss: 1.1333, Train Acc: 59.77%\n",
      "   Clean Test Loss: 1.4319, Clean Acc: 51.84%\n",
      "   Corrupted Test Loss: 920.3001, Corrupted Acc: 11.37%\n",
      "Epoch [7/10] - Train Loss: 1.0646, Train Acc: 62.22%\n",
      "   Clean Test Loss: 1.4514, Clean Acc: 52.25%\n",
      "   Corrupted Test Loss: 603.2917, Corrupted Acc: 14.27%\n",
      "Epoch [8/10] - Train Loss: 0.9991, Train Acc: 64.29%\n",
      "   Clean Test Loss: 1.4733, Clean Acc: 53.08%\n",
      "   Corrupted Test Loss: 577.8803, Corrupted Acc: 14.10%\n",
      "Epoch [9/10] - Train Loss: 0.9358, Train Acc: 66.75%\n",
      "   Clean Test Loss: 1.5250, Clean Acc: 52.21%\n",
      "   Corrupted Test Loss: 610.6134, Corrupted Acc: 13.85%\n",
      "Epoch [10/10] - Train Loss: 0.8774, Train Acc: 68.80%\n",
      "   Clean Test Loss: 1.5587, Clean Acc: 52.96%\n",
      "   Corrupted Test Loss: 473.8942, Corrupted Acc: 14.84%\n"
     ]
    }
   ],
   "source": [
    "# Train the Model on CIFAR-10\n",
    "lr = 1e-3\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "train_clean = True\n",
    "corruption_type = 'fog'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1) Build model\n",
    "model = MLP(input_size=3*32*32, hidden_size=1024, num_classes=10).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train_loader_c = get_cifar10c_loader(corruption_type=corruption_type, severity=4, batch_size=batch_size)\n",
    "test_loader_c = get_cifar10c_loader(corruption_type=corruption_type, severity=4, batch_size=batch_size)\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    if train_clean:\n",
    "        train_loss, train_acc = train_one_epoch(model, device, trainloader, optimizer, criterion)\n",
    "    else:\n",
    "        train_loss, train_acc = train_one_epoch(model, device, train_loader_c, optimizer, criterion)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%\")\n",
    "\n",
    "    # Evaluate on both clean and corrupted (if available)\n",
    "    clean_loss, clean_acc = evaluate(model, device, testloader, criterion)\n",
    "    print(f\"   Clean Test Loss: {clean_loss:.4f}, Clean Acc: {clean_acc*100:.2f}%\")\n",
    "\n",
    "    c_loss, c_acc = evaluate(model, device, test_loader_c, criterion)\n",
    "    print(f\"   Corrupted Test Loss: {c_loss:.4f}, Corrupted Acc: {c_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Alibi Detect\n",
    "from alibi_detect.datasets import fetch_cifar10c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10CDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wrap X, y from fetch_cifar10c (N, 32, 32, 3) arrays into a PyTorch dataset.\n",
    "    Applies optional transforms or normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        super().__init__()\n",
    "        self.X = X   # shape: (N, 32, 32, 3)\n",
    "        self.y = y   # shape: (N,)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.X[idx]    # (32, 32, 3)\n",
    "        label = self.y[idx]\n",
    "\n",
    "        # Convert to float32 tensor and reorder to (C, H, W):\n",
    "        img_tensor = torch.from_numpy(img).float().permute(2, 0, 1)\n",
    "        \n",
    "        # Apply transform (e.g. normalization)\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img_tensor)\n",
    "\n",
    "        return img_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10c_dataloader(corruption='gaussian_noise', severity=1, batch_size=64):\n",
    "    \"\"\"\n",
    "    Fetch a single corruption type + severity from CIFAR-10-C via alibi_detect,\n",
    "    wrap in a DataLoader for training or evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Fetch the data -> (X, y) as NumPy arrays\n",
    "    X, y = fetch_cifar10c(\n",
    "        corruption=corruption,\n",
    "        severity=severity,\n",
    "        return_X_y=True\n",
    "    )\n",
    "    print(f\"Fetched CIFAR-10-C: corruption={corruption}, severity={severity}, shape={X.shape}\")\n",
    "\n",
    "    # 2) Define transforms (common CIFAR-10 normalization)\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std  = (0.2470, 0.2435, 0.2616)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    # 3) Create custom dataset and dataloader\n",
    "    dataset = CIFAR10CDataset(X, y, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = outputs.max(dim=1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, device, data_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(dim=1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Hyperparameters\n",
    "    corruption = \"gaussian_noise\"  # e.g. \"motion_blur\", \"jpeg_compression\", etc.\n",
    "    severity = 1                   # integer 1..5\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    lr = 1e-3\n",
    "\n",
    "    # Device setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1) Load CIFAR-10-C train data (for demonstration, we assume we \"train\" on the corrupted set)\n",
    "    train_loader = load_cifar10c_dataloader(\n",
    "        corruption=corruption,\n",
    "        severity=severity,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # If you'd like a different severity or corruption for \"test\", do it here:\n",
    "    # test_loader = load_cifar10c_dataloader(corruption='motion_blur', severity=3, batch_size=batch_size)\n",
    "    # or re-use the same train_loader as test_loader for demonstration\n",
    "    test_loader = train_loader\n",
    "\n",
    "    # 2) Build the MLP model\n",
    "    model = MLP(input_size=3*32*32, hidden_size=1024, num_classes=10).to(device)\n",
    "\n",
    "    # 3) Optimizer & loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 4) Training loop\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, device, train_loader, optimizer, criterion)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2%}\")\n",
    "\n",
    "        test_loss, test_acc = evaluate(model, device, test_loader, criterion)\n",
    "        print(f\"           Test Loss: {test_loss:.4f},  Test Acc: {test_acc:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wtr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
